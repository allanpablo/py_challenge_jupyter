{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7055f2c3",
   "metadata": {},
   "source": [
    "# üéØ Overfitting e Underfitting\n",
    "\n",
    "Este notebook demonstra visualmente os conceitos de:\n",
    "- Underfitting (modelo simples demais)\n",
    "- Overfitting (modelo complexo demais)\n",
    "Usando regress√£o polinomial com diferentes graus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Dados simulados\n",
    "np.random.seed(42)\n",
    "X = np.sort(np.random.rand(100, 1) * 10, axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.3, X.shape[0])\n",
    "\n",
    "# Split em treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d797c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree in [1, 4, 15]:\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    model = LinearRegression().fit(X_train_poly, y_train)\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(X_test, y_test, color='black', s=10, label='Real')\n",
    "    plt.scatter(X_test, y_pred, color='red', s=10, label=f'Predi√ß√£o (Grau {degree})')\n",
    "    plt.title(f'Regress√£o Polinomial - Grau {degree} | MSE: {mean_squared_error(y_test, y_pred):.2f}')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo com grau 4 (ajuste razo√°vel)\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X_poly, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Erro quadr√°tico m√©dio (cross-validated):\", -scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecb430",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è T√©cnicas para evitar overfitting e underfitting\n",
    "\n",
    "- **Regulariza√ß√£o (L1/L2)**: penaliza modelos complexos\n",
    "- **Redu√ß√£o de dimensionalidade (PCA)**\n",
    "- **Sele√ß√£o de features relevantes**\n",
    "- **Ensemble (RandomForest, XGBoost)**\n",
    "- **Valida√ß√£o cruzada (k-fold)**: avalia generaliza√ß√£o\n",
    "\n",
    "Essas t√©cnicas ajudam a encontrar o equil√≠brio ideal entre vi√©s e vari√¢ncia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58174a",
   "metadata": {},
   "source": [
    "## üß∞ Regulariza√ß√£o: Ridge (L2) e Lasso (L1)\n",
    "\n",
    "T√©cnicas que adicionam penalidade √† complexidade do modelo, ajudando a evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3191ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Reutilizando grau 15 (muito complexo)\n",
    "degree = 15\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y)\n",
    "\n",
    "models = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.001, max_iter=10000)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"{name} Regression (Grau {degree}) - MSE: {mse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c71c69",
   "metadata": {},
   "source": [
    "## üå≤ RandomForest: Ensemble que ajuda a reduzir overfitting\n",
    "\n",
    "Random Forest √© uma t√©cnica baseada em m√∫ltiplas √°rvores com bagging e controle de profundidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"RandomForest MSE:\", mean_squared_error(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca432c",
   "metadata": {},
   "source": [
    "## üß™ Conclus√£o\n",
    "\n",
    "- Modelos simples causam underfitting\n",
    "- Modelos complexos demais overfitam\n",
    "- Regulariza√ß√£o e ensemble s√£o formas eficazes de controle\n",
    "- Valida√ß√£o cruzada √© essencial para medir generaliza√ß√£o\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
